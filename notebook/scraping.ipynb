{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import logging\n",
    "import csv  # Mengimpor csv untuk menyimpan data ke file CSV.\n",
    "\n",
    "# Konfigurasi logging\n",
    "logging.basicConfig(filename='scraping.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Kode warna\n",
    "GREEN = \"\\033[92m\"  # Warna hijau\n",
    "RED = \"\\033[91m\"    # Warna merah\n",
    "RESET = \"\\033[0m\"   # Reset ke warna default\n",
    "\n",
    "def scrape_kompas(pages: int, csv_filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Melakukan scraping berita dari situs Kompas dan menyimpan hasilnya ke file CSV.\n",
    "\n",
    "    Args:\n",
    "        pages (int): Jumlah halaman yang akan di-scrape.\n",
    "        csv_filename (str): Nama file CSV untuk menyimpan hasil.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Header CSV\n",
    "    fieldnames = ['judul', 'topik', 'sub_topik', 'topik_pilihan', 'tanggal_waktu_publish', 'redaksi', 'advetorial', 'isi_berita', 'link', 'topik_pilihan_link']\n",
    "    \n",
    "    # Membuka file CSV dalam mode append ('a') dan menulis header jika file baru\n",
    "    with open(csv_filename, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        \n",
    "        # Menulis header hanya jika file baru (dengan mengecek apakah filenya kosong)\n",
    "        if file.tell() == 0:\n",
    "            writer.writeheader()\n",
    "        \n",
    "        # Loop untuk setiap halaman dari 1 hingga pages\n",
    "        for i in range(1, pages + 1):\n",
    "            try:\n",
    "                url = f\"https://indeks.kompas.com/?site=all&page={i}\"\n",
    "                response = requests.get(url)\n",
    "                \n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                links = soup.find_all('a', class_='article-link')\n",
    "                links = [link.get('href') for link in links]\n",
    "                \n",
    "                # Loop untuk setiap link artikel yang ditemukan\n",
    "                for j, link in enumerate(links):\n",
    "                    try:\n",
    "                        response_news = BeautifulSoup(requests.get(link).text, 'html.parser')\n",
    "\n",
    "                        # Cek apakah artikel adalah advertorial\n",
    "                        try:\n",
    "                            advetorial = response_news.find('div', class_='kcm__header__advertorial').get_text()\n",
    "                        except:\n",
    "                            advetorial = ''\n",
    "\n",
    "                        # Mengambil topik dari breadcrumb\n",
    "                        try:\n",
    "                            topic_tags = response_news.find_all('li', class_='breadcrumb__item')\n",
    "                            topics = [tag.find('span').get_text() for tag in topic_tags]\n",
    "                            topik = topics[1]\n",
    "                        except:\n",
    "                            topik = ''\n",
    "\n",
    "                        # Mengambil sub-topik dari breadcrumb\n",
    "                        try:\n",
    "                            sub_topik = topics[2]\n",
    "                        except:\n",
    "                            sub_topik = ''\n",
    "\n",
    "                        # Mengambil topik pilihan jika ada\n",
    "                        try:\n",
    "                            topik_pilihan = response_news.find('div', class_='topicSubtitle').find('a').get_text()\n",
    "                            topik_pilihan_link = response_news.find('div', class_='topicSubtitle').find('a').get('href')\n",
    "                        except:\n",
    "                            topik_pilihan = ''\n",
    "                            topik_pilihan_link = ''\n",
    "\n",
    "                        # Mengambil judul artikel\n",
    "                        try:\n",
    "                            judul = response_news.find('h1', class_='read__title').get_text()\n",
    "                        except:\n",
    "                            judul = ''\n",
    "\n",
    "                        # Mengambil tanggal dan waktu publikasi\n",
    "                        try:\n",
    "                            tanggal_waktu_publish = response_news.find('div', class_='read__time').get_text().split(' - ')[1]\n",
    "                        except:\n",
    "                            tanggal_waktu_publish = ''\n",
    "\n",
    "                        # Mengambil nama redaksi yang menulis artikel\n",
    "                        try:\n",
    "                            redaksi_tag = response_news.find('div', class_='credit-title-name').find_all('h6')\n",
    "                            redaksi = [penulis.get_text() for penulis in redaksi_tag]\n",
    "                            redaksi = ' '.join(redaksi)\n",
    "                        except:\n",
    "                            redaksi = ''\n",
    "\n",
    "                        # Mengambil isi berita\n",
    "                        try:\n",
    "                            konteks_tag = response_news.find('div', class_='read__content').find_all('p')\n",
    "                            isi_berita = ' '.join([konteks.get_text() for konteks in konteks_tag])\n",
    "                        except:\n",
    "                            isi_berita = ''\n",
    "\n",
    "                        # Simpan hasil scraping ke dalam dictionary\n",
    "                        result = {\n",
    "                            'judul': judul,\n",
    "                            'topik': topik,\n",
    "                            'sub_topik': sub_topik,\n",
    "                            'topik_pilihan': topik_pilihan,\n",
    "                            'tanggal_waktu_publish': tanggal_waktu_publish,\n",
    "                            'redaksi': redaksi,\n",
    "                            'advetorial': advetorial,\n",
    "                            'isi_berita': isi_berita,\n",
    "                            'link': link,\n",
    "                            'topik_pilihan_link': topik_pilihan_link\n",
    "                        }\n",
    "\n",
    "                        # Menulis hasil ke file CSV\n",
    "                        writer.writerow(result)\n",
    "\n",
    "                        random_delay = random.uniform(0.1, 1)\n",
    "                        time.sleep(random_delay)\n",
    "                    except:\n",
    "                        print(f'{RED}ERROR - page {i} link {j + 1}  = {link}')\n",
    "                        logging.info(f'ERROR - page {i} link {j + 1}  = {link}')\n",
    "                        random_delay = random.uniform(0.1, 1)\n",
    "                        time.sleep(random_delay)\n",
    "                print()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f'{RED}ERROR - page {i}\\n')\n",
    "                logging.info(f'ERROR - page {i}\\n')\n",
    "                continue\n",
    "\n",
    "# Contoh pemanggilan fungsi\n",
    "scrape_kompas(9, 'scraping_result.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
