{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Menambahkan folder project ke sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "from src.validation.validate_data import validation_process \n",
    "# Menggunakan fungsi\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "import csv  # Mengimpor csv untuk menyimpan data ke file CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mERROR - page 1 link 5  = https://www.kompas.com/food/read/2024/09/08/181807975/cara-membuat-buttercream-sendiri-di-rumah-cuma-butuh-5-bahan\n"
     ]
    }
   ],
   "source": [
    "# Konfigurasi logging\n",
    "logging.basicConfig(filename='../data_source/scraping_data/scraping.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Kode warna\n",
    "GREEN = \"\\033[92m\"  # Warna hijau\n",
    "RED = \"\\033[91m\"    # Warna merah\n",
    "RESET = \"\\033[0m\"   # Reset ke warna default\n",
    "\n",
    "def scrape_kompas(pages: int, csv_filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Melakukan scraping berita dari situs Kompas dan menyimpan hasilnya ke file CSV.\n",
    "\n",
    "    Args:\n",
    "        pages (int): Jumlah halaman yang akan di-scrape.\n",
    "        csv_filename (str): Nama file CSV untuk menyimpan hasil.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Header CSV\n",
    "    fieldnames = ['judul', 'topik', 'sub_topik', 'topik_pilihan', 'tanggal_waktu_publish', 'redaksi', 'advetorial', 'isi_berita', 'link', 'topik_pilihan_link']\n",
    "    \n",
    "    # Membuka file CSV dalam mode append ('a') dan menulis header jika file baru\n",
    "    with open(csv_filename, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        \n",
    "        # Menulis header hanya jika file baru (dengan mengecek apakah filenya kosong)\n",
    "        if file.tell() == 0:\n",
    "            writer.writeheader()\n",
    "        \n",
    "        # Loop untuk setiap halaman dari 1 hingga pages\n",
    "        for i in range(1, pages + 1):\n",
    "            try:\n",
    "                url = f\"https://indeks.kompas.com/?site=all&page={i}\"\n",
    "                response = requests.get(url)\n",
    "                \n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                links = soup.find_all('a', class_='article-link')\n",
    "                links = [link.get('href') for link in links]\n",
    "                \n",
    "                # Loop untuk setiap link artikel yang ditemukan\n",
    "                for j, link in enumerate(links):\n",
    "                    try:\n",
    "                        response_news = BeautifulSoup(requests.get(link).text, 'html.parser')\n",
    "\n",
    "                        # Cek apakah artikel adalah advertorial\n",
    "                        try:\n",
    "                            advetorial = response_news.find('div', class_='kcm__header__advertorial').get_text()\n",
    "                        except:\n",
    "                            advetorial = ''\n",
    "\n",
    "                        # Mengambil topik dari breadcrumb\n",
    "                        try:\n",
    "                            topic_tags = response_news.find_all('li', class_='breadcrumb__item')\n",
    "                            topics = [tag.find('span').get_text() for tag in topic_tags]\n",
    "                            topik = topics[1]\n",
    "                        except:\n",
    "                            topik = ''\n",
    "\n",
    "                        # Mengambil sub-topik dari breadcrumb\n",
    "                        try:\n",
    "                            sub_topik = topics[2]\n",
    "                        except:\n",
    "                            sub_topik = ''\n",
    "\n",
    "                        # Mengambil topik pilihan jika ada\n",
    "                        try:\n",
    "                            topik_pilihan = response_news.find('div', class_='topicSubtitle').find('a').get_text()\n",
    "                            topik_pilihan_link = response_news.find('div', class_='topicSubtitle').find('a').get('href')\n",
    "                        except:\n",
    "                            topik_pilihan = ''\n",
    "                            topik_pilihan_link = ''\n",
    "\n",
    "                        # Mengambil judul artikel\n",
    "                        try:\n",
    "                            judul = response_news.find('h1', class_='read__title').get_text()\n",
    "                        except:\n",
    "                            judul = ''\n",
    "\n",
    "                        # Mengambil tanggal dan waktu publikasi\n",
    "                        try:\n",
    "                            tanggal_waktu_publish = response_news.find('div', class_='read__time').get_text().split(' - ')[1]\n",
    "                        except:\n",
    "                            tanggal_waktu_publish = ''\n",
    "\n",
    "                        # Mengambil nama redaksi yang menulis artikel\n",
    "                        try:\n",
    "                            redaksi_tag = response_news.find('div', class_='credit-title-name').find_all('h6')\n",
    "                            redaksi = [penulis.get_text() for penulis in redaksi_tag]\n",
    "                            redaksi = ' '.join(redaksi)\n",
    "                        except:\n",
    "                            redaksi = ''\n",
    "\n",
    "                        # Mengambil isi berita\n",
    "                        try:\n",
    "                            konteks_tag = response_news.find('div', class_='read__content').find_all('p')\n",
    "                            isi_berita = ' '.join([konteks.get_text() for konteks in konteks_tag])\n",
    "                        except:\n",
    "                            isi_berita = ''\n",
    "\n",
    "                        # Simpan hasil scraping ke dalam dictionary\n",
    "                        result = {\n",
    "                            'judul': judul,\n",
    "                            'topik': topik,\n",
    "                            'sub_topik': sub_topik,\n",
    "                            'topik_pilihan': topik_pilihan,\n",
    "                            'tanggal_waktu_publish': tanggal_waktu_publish,\n",
    "                            'redaksi': redaksi,\n",
    "                            'advetorial': advetorial,\n",
    "                            'isi_berita': isi_berita,\n",
    "                            'link': link,\n",
    "                            'topik_pilihan_link': topik_pilihan_link\n",
    "                        }\n",
    "\n",
    "                        # Menulis hasil ke file CSV\n",
    "                        writer.writerow(result)\n",
    "\n",
    "                        random_delay = random.uniform(0.1, 1)\n",
    "                        time.sleep(random_delay)\n",
    "                    except:\n",
    "                        print(f'{RED}ERROR - page {i} link {j + 1}  = {link}')\n",
    "                        logging.info(f'ERROR - page {i} link {j + 1}  = {link}')\n",
    "                        random_delay = random.uniform(0.1, 1)\n",
    "                        time.sleep(random_delay)\n",
    "                print()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f'{RED}ERROR - page {i}\\n')\n",
    "                logging.info(f'ERROR - page {i}\\n')\n",
    "                continue\n",
    "\n",
    "# Contoh pemanggilan fungsi\n",
    "scrape_kompas(5, '../data_source/scraping_data/scraping_kompas.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data_source/scraping_data/scraping_kompas.csv')\n",
    "data_name = 'scraping_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Start scraping_data Pipeline Validation ==========\n",
      "CHECKING SHAPE DATA\n",
      "Data scraping_data has 583 rows and 10 columns\n",
      "\n",
      "CHECKING DATA TYPE\n",
      "Each column in scraping_data has the following data types:\n",
      "\n",
      "Column `judul` has data type object\n",
      "Column `topik` has data type object\n",
      "Column `sub_topik` has data type object\n",
      "Column `topik_pilihan` has data type object\n",
      "Column `tanggal_waktu_publish` has data type object\n",
      "Column `redaksi` has data type object\n",
      "Column `advetorial` has data type object\n",
      "Column `isi_berita` has data type object\n",
      "Column `link` has data type object\n",
      "Column `topik_pilihan_link` has data type object\n",
      "\n",
      "CHECKING MISSING DATA\n",
      "Each column in `scraping_data` has the following number of missing values:\n",
      "\n",
      "Column `judul` has 0 or 0.0% missing values\n",
      "Column `topik` has 9 or 1.5% missing values\n",
      "Column `sub_topik` has 257 or 44.1% missing values\n",
      "Column `topik_pilihan` has 537 or 92.1% missing values\n",
      "Column `tanggal_waktu_publish` has 0 or 0.0% missing values\n",
      "Column `redaksi` has 9 or 1.5% missing values\n",
      "Column `advetorial` has 560 or 96.1% missing values\n",
      "Column `isi_berita` has 0 or 0.0% missing values\n",
      "Column `link` has 0 or 0.0% missing values\n",
      "Column `topik_pilihan_link` has 537 or 92.1% missing values\n",
      "\n",
      "CHECKING DATA DUPLICATES\n",
      "`scraping_data` has 160 duplicate entries \n",
      "\n",
      "CHECKING UNIQUE VALUES\n",
      "Each column in `scraping_data` has the following number of missing values:\n",
      "\n",
      "Jumlah unique values pada kolom `judul` adalah sebanyak : 421\n",
      "\n",
      "Jumlah unique values pada kolom `topik `adalah sebanyak : 23 yaitu : \n",
      " ['Regional' 'Money' 'News' 'Otomotif' 'Tekno' 'Edu' 'Tren' 'Cek fakta'\n",
      " 'Hype' 'Bola' 'Global' 'Food' 'Lifestyle' 'Properti' 'Health' nan\n",
      " 'Travel' 'Lestari' 'Internasional' 'Ikn' 'Umkm' 'Homey' 'Stori']\n",
      "\n",
      "Jumlah unique values pada kolom `sub_topik `adalah sebanyak : 50 yaitu : \n",
      " [nan 'Ekbis' 'Megapolitan' 'Mobil' 'News' 'Nasional' 'Apps & OS'\n",
      " 'Keuangan' 'Hoaks atau Fakta' 'Timnas Indonesia' 'Feature'\n",
      " 'Liga Indonesia' 'Resep' 'Wellness' 'Hardware' 'Sport' 'Berita' 'Gadget'\n",
      " 'Travel Update' 'Fashion' 'LSM/Figur' 'Pemerintah' 'e-Business' 'Rilis'\n",
      " 'Sports' 'Energi' 'Smartpreneur' 'Tempat Makan' 'Swasta' 'Belanja'\n",
      " 'Training' 'Tips Kuliner' 'Karier' 'Game' 'Perumahan' 'Motogp'\n",
      " 'Internasional' 'Food News' 'Kawasan Terpadu' 'Industri' 'Liga Italia'\n",
      " 'Do it your self' 'Interior' 'Motor' 'Decor' 'Tips' 'Parenting'\n",
      " 'Internet' 'Umum' 'Cuan']\n",
      "\n",
      "Jumlah unique values pada kolom `topik_pilihan `adalah sebanyak : 20 yaitu : \n",
      " [nan 'Pilkada 2024' 'Berita Viral, Hoaks atau Fakta?'\n",
      " 'Paus Fransiskus ke Indonesia' 'Jakarta Maju Bersama' 'Bandung Bedas'\n",
      " 'Ketahanan Pangan Nasional' 'Infografis Kompas.com'\n",
      " 'IKN dan Pemindahan Ibu Kota Baru' 'MotoGP San Marino 2024'\n",
      " 'Kediri Hari Ini ' 'Semarang Hebat ' 'Serat Rayon Ramah Lingkungan'\n",
      " 'Oli Sintetik Masa Kini' 'Pilihan Rakyat ' 'Blora Mustika'\n",
      " 'Perang Rusia vs Ukraina' 'KUKAR Idaman Terbaik' 'Medan Berkah'\n",
      " 'Timnas Indonesia di Putaran 3 Kualifikasi Piala Dunia 2026']\n",
      "\n",
      "Jumlah unique values pada kolom `tanggal_waktu_publish` adalah sebanyak : 299\n",
      "\n",
      "Jumlah unique values pada kolom `redaksi` adalah sebanyak : 217\n",
      "\n",
      "Jumlah unique values pada kolom `advetorial `adalah sebanyak : 7 yaitu : \n",
      " [nan 'Advertorial' 'KILAS METRO' 'KILAS DAERAH' 'KILAS' 'RILIS BIZ'\n",
      " 'SOROT POLITIK']\n",
      "\n",
      "Jumlah unique values pada kolom `isi_berita` adalah sebanyak : 423\n",
      "\n",
      "Jumlah unique values pada kolom `link` adalah sebanyak : 421\n",
      "\n",
      "Jumlah unique values pada kolom `topik_pilihan_link `adalah sebanyak : 20 yaitu : \n",
      " [nan 'https://indeks.kompas.com/topik-pilihan/list/9032/pilkada-2024'\n",
      " 'https://indeks.kompas.com/topik-pilihan/list/4390/berita-viral-hoaks-atau-fakta'\n",
      " 'https://indeks.kompas.com/topik-pilihan/list/2358/paus-fransiskus-ke-indonesia'\n",
      " 'https://indeks.kompas.com/topik-pilihan/list/5708/jakarta-maju-bersama'\n",
      " 'https://indeks.kompas.com/topik-pilihan/list/8639/bandung-bedas'\n",
      " 'https://indeks.kompas.com/topik-pilihan/list/4714/ketahanan-pangan-nasional'\n",
      " 'https://indeks.kompas.com/topik-pilihan/list/5348/infografis-kompascom'\n",
      " 'https://indeks.kompas.com/topik-pilihan/list/6920/ikn-dan-pemindahan-ibu-kota-baru'\n",
      " 'https://indeks.kompas.com/topik-pilihan/list/9179/motogp-san-marino-2024'\n",
      " 'https://indeks.kompas.com/topik-pilihan/list/8267/kediri-hari-ini'\n",
      " 'https://indeks.kompas.com/topik-pilihan/list/4579/semarang-hebat'\n",
      " 'https://indeks.kompas.com/topik-pilihan/list/6214/serat-rayon-ramah-lingkungan'\n",
      " 'https://indeks.kompas.com/topik-pilihan/list/9236/oli-sintetik-masa-kini'\n",
      " 'https://indeks.kompas.com/topik-pilihan/list/8723/pilihan-rakyat'\n",
      " 'https://indeks.kompas.com/topik-pilihan/list/7331/blora-mustika'\n",
      " 'https://indeks.kompas.com/topik-pilihan/list/6959/perang-rusia-vs-ukraina'\n",
      " 'https://indeks.kompas.com/topik-pilihan/list/9233/kukar-idaman-terbaik'\n",
      " 'https://indeks.kompas.com/topik-pilihan/list/6640/medan-berkah'\n",
      " 'https://indeks.kompas.com/topik-pilihan/list/9147/timnas-indonesia-di-putaran-3-kualifikasi-piala-dunia-2026']\n",
      "\n",
      "========== End Pipeline Validation ==========\n"
     ]
    }
   ],
   "source": [
    "validation_process(df, data_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Misiing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_scraping_data(df_scraping):\n",
    "\n",
    "    # Isi data topik kosong menjadi iklan karena berita merupakan beritak iklan (advetorial=advertorial)\n",
    "    df_scraping['topik'] = df_scraping['topik'].fillna('Iklan')\n",
    "\n",
    "    # Isi data subtopik yg kosong dengan \"Belum ditentukan\"\n",
    "    df_scraping['sub_topik'] = df_scraping['sub_topik'].fillna('Belum ditentukan')\n",
    "\n",
    "    # Mengisi data kosong topik pilihan dengan bukan topik pilihan\n",
    "    df_scraping['topik_pilihan'] = df_scraping['topik_pilihan'].fillna('Bukan topik pilihan')\n",
    "\n",
    "    # Redaksi yang kosong dianggap anonim\n",
    "    df_scraping['redaksi'] = df_scraping['redaksi'].fillna('Anonim')\n",
    "\n",
    "    df_scraping['advetorial'] = df_scraping['advetorial'].fillna('Non Advertorial')\n",
    "\n",
    "    # Menghapus \"WIB\" dan konversi menjadi datetime\n",
    "    df_scraping['tanggal_waktu_publish'] = df_scraping['tanggal_waktu_publish'].str.replace(' WIB', '')\n",
    "    df_scraping['tanggal_waktu_publish'] = pd.to_datetime(df_scraping['tanggal_waktu_publish'], format='%d/%m/%Y, %H:%M')\n",
    "\n",
    "    # Drop topik pilinan link\n",
    "    df_scraping = df_scraping.drop('topik_pilihan_link',axis=1)\n",
    "\n",
    "    return df_scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>judul</th>\n",
       "      <th>topik</th>\n",
       "      <th>sub_topik</th>\n",
       "      <th>topik_pilihan</th>\n",
       "      <th>tanggal_waktu_publish</th>\n",
       "      <th>redaksi</th>\n",
       "      <th>advetorial</th>\n",
       "      <th>isi_berita</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dosen di Solo Terjerat Kasus Penipuan Jual Bel...</td>\n",
       "      <td>Regional</td>\n",
       "      <td>Belum ditentukan</td>\n",
       "      <td>Bukan topik pilihan</td>\n",
       "      <td>2024-09-06 16:46:00</td>\n",
       "      <td>Rachmawati</td>\n",
       "      <td>Non Advertorial</td>\n",
       "      <td>KOMPAS.com - H, salah satu dosen di Solo, Jawa...</td>\n",
       "      <td>https://regional.kompas.com/read/2024/09/06/16...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               judul     topik  \\\n",
       "0  Dosen di Solo Terjerat Kasus Penipuan Jual Bel...  Regional   \n",
       "\n",
       "          sub_topik        topik_pilihan tanggal_waktu_publish     redaksi  \\\n",
       "0  Belum ditentukan  Bukan topik pilihan   2024-09-06 16:46:00  Rachmawati   \n",
       "\n",
       "        advetorial                                         isi_berita  \\\n",
       "0  Non Advertorial  KOMPAS.com - H, salah satu dosen di Solo, Jawa...   \n",
       "\n",
       "                                                link  \n",
       "0  https://regional.kompas.com/read/2024/09/06/16...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result = transform_scraping_data(df.copy())\n",
    "# validation_process(transform_scraping_data(df), data_name)\n",
    "df_result[df_result['judul'].str.contains('Tawarkan 1 Kaveling ke Banyak Pembeli')]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
